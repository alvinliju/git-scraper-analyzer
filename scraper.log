Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 860 already scraped repos
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 25, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 17, in run_scraper
    print(f"✓ Scraped {len(details)} repo details")
                       ~~~^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
Starting to scrape github repos...
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connection.py", line 198, in _new_conn
    sock = connection.create_connection(
        (self._dns_host, self.port),
    ...<2 lines>...
        socket_options=self.socket_options,
    )
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/util/connection.py", line 60, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
               ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py", line 977, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
               ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
socket.gaierror: [Errno 8] nodename nor servname provided, or not known

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
        conn,
    ...<10 lines>...
        **response_kw,
    )
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 488, in _make_request
    raise new_e
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 464, in _make_request
    self._validate_conn(conn)
    ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 1093, in _validate_conn
    conn.connect()
    ~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connection.py", line 753, in connect
    self.sock = sock = self._new_conn()
                       ~~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connection.py", line 205, in _new_conn
    raise NameResolutionError(self.host, self, e) from e
urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x10a7816a0>: Failed to resolve 'api.github.com' ([Errno 8] nodename nor servname provided, or not known)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/requests/adapters.py", line 644, in send
    resp = conn.urlopen(
        method=request.method,
    ...<9 lines>...
        chunked=chunked,
    )
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
        method, url, error=new_e, _pool=self, _stacktrace=sys.exc_info()[2]
    )
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/urllib3/util/retry.py", line 519, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.github.com', port=443): Max retries exceeded with url: /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=1 (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x10a7816a0>: Failed to resolve 'api.github.com' ([Errno 8] nodename nor servname provided, or not known)"))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 25, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 9, in run_scraper
    repos = scrape_all_github_repos()
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/main.py", line 52, in scrape_all_github_repos
    response = scrape_github_repos_with_page(page)
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/main.py", line 36, in scrape_github_repos_with_page
    response = requests.get(url, headers=headers)
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/requests/api.py", line 73, in get
    return request("get", url, params=params, **kwargs)
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/requests/api.py", line 59, in request
    return session.request(method=method, url=url, **kwargs)
           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/requests/adapters.py", line 677, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.github.com', port=443): Max retries exceeded with url: /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=1 (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x10a7816a0>: Failed to resolve 'api.github.com' ([Errno 8] nodename nor servname provided, or not known)"))
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 888 already scraped repos
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 25, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 17, in run_scraper
    print(f"✓ Scraped {len(details)} repo details")
                       ~~~^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 25, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 17, in run_scraper
    print(f"✓ Scraped {len(details)} repo details")
                       ~~~^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 25, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 17, in run_scraper
    print(f"✓ Scraped {len(details)} repo details")
                       ~~~^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 25, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 17, in run_scraper
    print(f"✓ Scraped {len(details)} repo details")
                       ~~~^^^^^^^^^
TypeError: object of type 'NoneType' has no len()
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Data saved to github_repos
✓ Found 900 repos
Skipping 0 already scraped repos
<class 'NoneType'>
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 29, in <module>
    asyncio.run(run_scraper())
    ~~~~~~~~~~~^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 195, in run
    return runner.run(main)
           ~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py", line 725, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 21, in run_scraper
    print(f"✓ Scraped {len(filtered_details)} repo details")
                           ^^^^^^^^^^^^^^^^
NameError: name 'filtered_details' is not defined
2025-11-10 12:00:02,521 - DEBUG - Using selector: KqueueSelector
2025-11-10 12:00:02,571 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:07,131 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=1 HTTP/1.1" 200 None
2025-11-10 12:00:07,361 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:11,007 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=2 HTTP/1.1" 200 None
2025-11-10 12:00:11,439 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:14,927 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=3 HTTP/1.1" 200 None
2025-11-10 12:00:15,336 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:19,145 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=4 HTTP/1.1" 200 None
2025-11-10 12:00:19,564 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:23,520 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=5 HTTP/1.1" 200 None
2025-11-10 12:00:23,746 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:27,676 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=6 HTTP/1.1" 200 None
2025-11-10 12:00:27,858 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:31,869 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=7 HTTP/1.1" 200 None
2025-11-10 12:00:32,097 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:35,496 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=8 HTTP/1.1" 200 None
2025-11-10 12:00:35,885 - DEBUG - Starting new HTTPS connection (1): api.github.com:443
2025-11-10 12:00:39,344 - DEBUG - https://api.github.com:443 "GET /search/repositories?q=stars:%3E1000&sort=stars&order=desc&per_page=100&page=9 HTTP/1.1" 200 None
2025-11-10 12:00:39,579 - DEBUG - [SAVE RAW] Starting to save 900 repos to database
2025-11-10 12:00:39,580 - DEBUG - [SAVE RAW] Sample repo keys: ['id', 'node_id', 'name', 'full_name', 'private', 'owner', 'html_url', 'description', 'fork', 'url']
2025-11-10 12:00:39,580 - DEBUG - [SAVE RAW] Sample repo - Name: build-your-own-x, HTML URL: https://github.com/codecrafters-io/build-your-own-x
2025-11-10 12:00:39,926 - DEBUG - [SAVE RAW] Successfully committed 900 repos, skipped 0
2025-11-10 12:00:39,928 - DEBUG - [SAVE RAW] Total records in raw_repos table after save: 889
Starting to scrape github repos...
Page 1 scraped successfully
Starting to scrape github repos...
Page 2 scraped successfully
Starting to scrape github repos...
Page 3 scraped successfully
Starting to scrape github repos...
Page 4 scraped successfully
Starting to scrape github repos...
Page 5 scraped successfully
Starting to scrape github repos...
Page 6 scraped successfully
Starting to scrape github repos...
Page 7 scraped successfully
Starting to scrape github repos...
Page 8 scraped successfully
Starting to scrape github repos...
Page 9 scraped successfully
Raw repos saved to db: 900 (skipped: 0)
✓ Found 900 repos
Skipping 889 already scraped repos
All repos already scraped!
No repos to scrape
Traceback (most recent call last):
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/cron_job.py", line 3, in <module>
    from main import scrape_all_github_repos, save_all_github_repos, run_repo_scans_in_parallel
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/main.py", line 15, in <module>
    conn = psycopg2.connect(
        host="localhost",
    ...<3 lines>...
        port=5432,
    )
  File "/Users/l3mon/Documents/finaltryworked/gitscraper/myenv/lib/python3.13/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (::1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

